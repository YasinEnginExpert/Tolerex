Distributed computing is a field of computer science that studies distributed systems, defined as computer systems whose inter-communicating components are located on different networked computers.[1][2]
The components of a distributed system communicate and coordinate their actions by passing messages to one another in order to achieve a common goal. Three challenges of distributed systems are: maintaining concurrency of components, overcoming the lack of a global clock, and managing the independent failure of components.[1] When a component of one system fails, the entire system does not fail.[3] Examples of distributed systems vary from SOA-based systems to microservices to massively multiplayer online games to peer-to-peer applications. Distributed systems cost more than monolithic architectures, primarily due to increased needs for additional hardware, servers, gateways, firewalls, new subnets, proxies, and so on.[4] Distributed systems can also suffer from fallacies of distributed computing. Conversely, a well-designed distributed system is more scalable, more durable, more changeable, and more fine-tuned than a monolithic application deployed on a single machine.[5] According to Marc Brooker: "a system is scalable in the range where marginal cost of additional workload is nearly constant." Serverless technologies fit this definition but the total cost of ownership, and not just the infra cost must be considered.[6]
A computer program that runs within a distributed system is called a distributed program,[7] and distributed programming is the process of writing such programs.[8] There are many types of implementations for the message-passing mechanism, including pure HTTP, RPC-like connectors, and message queues.[9]
Distributed computing also refers to the use of distributed systems to solve computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers,[10] which communicate with each other via message passing.[11]
Introduction
The word distributed in terms such as "distributed system", "distributed programming", and "distributed algorithm" originally referred to computer networks where individual computers were physically distributed within some geographical area.[12] The terms are nowadays used in a much wider sense, even referring to autonomous processes that run on the same physical computer and interact with each other by message passing.[11]
There is no single definition of a distributed system,[13] but two common properties are generally cited:
There are several autonomous computational entities (computers or nodes), each of which has its own local memory.[14]
The entities communicate with each other by message passing.[15]
A distributed system may have a common goal, such as solving a large computational problem;[16] the user then perceives the collection of autonomous processors as a unit. Alternatively, each computer may have its own user with individual needs, and the purpose of the distributed system is to coordinate the use of shared resources or provide communication services to the users.[17]
Other typical properties of distributed systems are:
The system must tolerate failures in individual computers.[18]
The structure of the system (network topology, network latency, number of computers) is not known in advance.
The system may consist of different kinds of computers and network links.
The system may change during the execution of a distributed program.[19]
Each computer has a limited, incomplete view of the system.
Each computer may know only one part of the input.[20]
Patterns
Here are common architectural patterns used for distributed computing:[21]
Saga interaction pattern
Microservices
Event driven architecture
Client–server architecture
Service-oriented architecture (SOA)
Publish–subscribe pattern
Peer-to-peer (P2P)
Events vs. Messages
In distributed systems, events represent a fact or state change (e.g., OrderPlaced) and are typically broadcast asynchronously to multiple consumers, promoting loose coupling and scalability. While events generally don't expect an immediate response, acknowledgment mechanisms are often implemented at the infrastructure level (e.g., Kafka commit offsets, SNS delivery statuses) rather than being an inherent part of the event pattern itself.[22][23]
In contrast, messages serve a broader role, encompassing commands (e.g., ProcessPayment), events (e.g., PaymentProcessed), and documents (e.g., DataPayload). Both events and messages can support various delivery guarantees, including at-least-once, at-most-once, and exactly-once, depending on the technology stack and implementation. However, exactly-once delivery is often achieved through idempotency mechanisms rather than true, infrastructure-level exactly-once semantics.[22][23]
Delivery patterns for both events and messages include publish/subscribe (one-to-many) and point-to-point (one-to-one). While request/reply is technically possible, it is more commonly associated with messaging patterns rather than pure event-driven systems. Events excel at state propagation and decoupled notifications, while messages are better suited for command execution, workflow orchestration, and explicit coordination.[22][23]
Modern architectures commonly combine both approaches, leveraging events for distributed state change notifications and messages for targeted command execution and structured workflows based on specific timing, ordering, and delivery requirements.[22][23]